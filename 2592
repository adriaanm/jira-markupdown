I've had a chance to look at this now, and I have good news and bad news.

The good news is that not only is it possible to speed up Murmur2 as applied to
case classes, but it appears you can make Murmur2 substantially
faster than the current default hash code.

The bad news is that the amount of work required is nontrivial and
not readily generalizable
to other hash algorithms, should one be found that is preferable to Murmur2.

The other news is that Murmur2 is not necessarily the ideal target;
since making the entry above, I've benchmarked Murmur2 and Bob Jenkins's
lookup3 on a couple of other machines, and at least on an Atom processor,
lookup3 is faster than Murmur2 (by about the same ratio as Murmur2 is faster
than lookup3 on the other machines where I tested it).
It may be that the Atom is not representative of the sort of machine where
people run Scala code, but it's good to keep in mind that performance is a
slippery thing, and always context-dependent.

== How to Make It Fast: an Idea ==

The key to speeding up a structural (that is, not identity-based) hash
algorithm is to recognize it as a kind of fold. That is, you'd like to add to
Product something like:
```scala
    def foldProductElementsLeft [B] (zero: B) (op: (B, Any) => B): B = {
      var x = zero
      for (a <- productIterator)
        x = op(x,a)
      x
    }
```

and apply it to your favorite hash function (such as
`(h: Int, x: Any) => h * 41 + (if (x == null) 0 else x.##)`,
if you want the original case class hash function).

However, if you make the *compiler* generate
`foldProductElementsLeft` for each case class,
it can unroll the loop. For example, for:
```scala
  case class C (
      z: Boolean, b: Byte, c: Char, s: Short, i: Int, l: Long, f: Float,
      d: Double, str: String)
```

you could generate:
```scala
    def foldProductElementsLeft [B] (zero: B) (op: (B, Any) => B): B = {
      var x = zero
      x = op(x,z)
      x = op(x,b)
      x = op(x,c)
      x = op(x,s)
      x = op(x,i)
      x = op(x,l)
      x = op(x,f)
      x = op(x,d)
      x = op(x,str)
      x
    }
```

For the unrolled version of `foldProductElementsLeft`,
a sufficiently smart VM should
be able to elide the implied boxing and unboxing, which might be a good
deal harder in the loop-based hand-written version, unless the VM itself is
clever enough to unroll the loop.

If you think your VM is dumb as a board and you want to lead it by the nose,
you can explicitly eliminate boxing by reimagining a `Product` as a sequence of
field hash codes:
```scala
    def foldProductHashCodesLeft (zero: Int) (op: (Int, Int) => Int): Int = {
      var x = zero
      x = op(x,z.##)
      x = op(x,b.##)
      x = op(x,c.##)
      x = op(x,s.##)
      x = op(x,i.##)
      x = op(x,l.##)
      x = op(x,f.##)
      x = op(x,d.##)
      x = op(x,if (str == null) 0 else str.##)
      x
    }
```

But this is screwy in several ways:

 * You really want to hash the raw bits of each non-reference field, not its
  `##`, even though the latter is usually the same.

 * There isn't any point in hashing the leading zeros of primitive types smaller
  than `Int`; if possible, you should consolidate the smaller fields into
  `Int`s.

 * `Int` isn't necessarily the proper quantum for all hash algorithms. lookup3
  wants its input in 96-bit chunks, not 32-bit.

Addressing the first two problems yields something like:
```scala
    def foldProductRawBitsLeft (zero: Int) (op: (Int, Int) => Int): Int = {
      var x = zero
      x = op(x,(if (z) 1 << 16 else 0) | c)
      x = op(x,(b << 16) | (s & 0xffff))
      x = op(x,i)
      x = op(x,l.toInt)
      x = op(x,(l >>> 32).toInt)
      x = op(x,java.lang.Float.floatToRawIntBits(f))
      val dd = java.lang.Double.doubleToRawLongBits(d)
      x = op(x,dd.toInt)
      x = op(x,(dd >>> 32).toInt)
      x = op(x,if (str == null) 0 else str.##)
      x
    }
```

I have no clue how to address the last problem in any kind of general way
while avoiding object allocation. The best I can come up with is to generate
distinct fold methods for each hash quantum desired, but because
there's no guarantee
that some yet-to-be-invented superior hash function will want to consume data
in chunks of 32 or 96 bits like the functions we have now, this is a potentially
endless task.

== Idea Meets Cruel Reality: Benchmark Results ==

The bad news is that the JVM (1.6.0_21 for 32-bit Windows,
both client and server) is dumb as a board.
On my machine, for instances of the class `C` above compiled under
scala-2.9.0.r23230-b20101012020216,
times for hash algorithms that allocate short-lived objects appear to be
roughly proportional to the number of objects allocated, suggesting that
the JVM fails to eliminate any of the allocations.
In particular, a hash function based on a general-purpose fold is considerably
slower than the `Int`-based folds (and slower than the current default).

The good news is that if you do break down and generate an `Int`-based fold,
the fastest implementation of Murmur2
(the one based on `foldProductRawBitsLeft` above) benchmarks 3 to 4
times as fast as the current default hash algorithm (combining the
current hash function with `foldProductRawBitsLeft` is faster still, since
the former does fewer operations, but the result is still poorly distributed).

You can use the attached `HashSpeed.scala` to see whether timings on your
machine resemble mine.

I don't know how the timings in the benchmark would translate to the compiler,
but I gather from the fact that you were able to observe any slowdown by
changing the hash algorithm that the compiler spends a good deal of its time
hashing.
