As a new user of Scala, I was thrilled to find out that Scala had a way to generate those boring, repetitive implementations of equals, hashCode, and toString for me. Case classes rule!

I was not so thrilled when I checked the collision rate of those automatically generated case class hash codes. The following program should report approximately 1 collision:

{code}
object XTest {

    case class C (x: Int, y: Int) { }

    def main (args: Array[String]) = {
        var hashCodeCounts: Map[Int,Int] = Map()
        var repetitions = 0
        for (x <- 0 until 300; y <- 0 until 300) {
            val c = C(x,y)
            val h = c.hashCode
            val numTimesSeen = hashCodeCounts.getOrElse(h,0)
            if (numTimesSeen > 0) {
                System.out.println(
                    "Already saw hash code " + h + " for " + c + " " +
                        numTimesSeen + " times.")
                repetitions += 1
            }
            hashCodeCounts = hashCodeCounts.update(h,numTimesSeen + 1)
        }
        System.out.println("\n" + repetitions + " repetitions.\n")
    }

}
{code}

Instead, it reports 77,441.

A number of good algorithms for automatically generating hash codes come to mind. You might, for example, pass the bytes composing the name of the class and any non-reference fields through a high-quality bytestring hash algorithm (like Bob Jenkins's lookup3; see http://burtleburtle.net/bob/c/lookup3.c), and add the result to the sum of the hash codes from reference fields (this produces a well-distributed result if the reference field hash codes are themselves well-distributed).
Replying to [comment:5 extempore]:
> Replying to [comment:1 archontophoenix]:
> > I should have specified that I'm using Scala version 2.7.7.RC2.
> 
> I checked in something like what you describe in r19392 but it's not enabled by default, you have to compile with -Yjenkins-hashCodes.  If you or anyone reading this would like to sanity check it, maybe if I collect a few approval stamps it'll sound good to switch for real.

I haven't tried running your code, but after looking at it I saw a couple of things you might want to do a little bit differently:

  * You seem to be doing some object allocation in computing the hash codes for non-refs, turning them into a `ByteBuffer`. Ideally, computing a hash code wouldn't involve any allocation.
  * Assuming that Scala's 32-bit integer semantics are the same as Java's, you don't need to use `Long` arithmetic to get the effect of unsigned C `int`s; ordinary `Int`s will work just fine.

The `Hash.java` file I've attached is from a not-yet-completed Java project where I'm planning to generate hash codes automatically for some classes. The approach I'm taking uses thread-safe immutable adapters (of type `intsFrom`) to wrap objects to make them look like a list of `int`s.
The plan (not yet carried out) is to generate an auxiliary subclass _A_ of `IntsFrom` for each class _C_ for which the hash code is generated automatically, and to allocate a single instance of _A_ and store it in a `static` variable in _C_, to use when `hashCode` is called. This approach allocates just one extra object per class loaded, not multiple objects per invocation of `hashCode`.
I have finally tried benchmarking the existing ScalaRunTime._hashCodeJenkins in 2.8.0.Beta1. It runs about 15 - 200 times (!) as slow as ScalaRunTime._hashCode. So I've tried a reimplemented Jenkins algorithm, as well as some other well-known hash algorithms.

Austin Appleby, author of MurmurHash2, ran some quality tests on several hash algorithms: MurmurHash2, Bob Jenkins's lookup3, FNV, and Paul Hsieh's SuperFastHash (at http://sites.google.com/site/murmurhash/statistics). Of these four algorithms, only MurmurHash2 and lookup3 performed well on all tests; FNV and SuperFastHash have distributions that are poor by at least some measures.

On my machine, MurmurHash2 runs faster than lookup3, by about 10% - 40%, depending on what is being hashed. MurmurHash2 uses lots of multiplies, and lookup3 lots of rotates, so different relative speeds of those operations might lead to different results on different machines.

I've coded both algorithms to take a Product argument and to avoid the allocation of temporary objects, other than those required for boxing when calling Product.productElement. Judging from the lack of apparent effect from the -XX:+DoEscapeAnalysis switch in JDK 6 update 17, it may be premature to expect VMs with escape analysis to eliminate concerns about such allocations in
time-critical code.

I also coded versions of the data structures I hashed to implement a trait that looks like this:

{code}
    trait Hashable {
      def hashableInts: Int
      def hashableInt (n: Int): Int
    }
{code}

and coded versions of the hash algorithms to call the methods in it to obtain the contents of the data structures as a sequence of 32-bit integers, thereby avoiding any allocations during hashing. As expected, this resulted in a substantial speedup, of as much as 500%. However, it is not clear that it would be worth changing Product (for example) to include such methods, particularly since a superior hash algorithm may one day appear that consumes its input in
something other than 32-bit chunks. (Scala should certainly avoid the mistake of java.lang.String, which promises a specific, and decidedly low-quality, hash function.)

Attached are implementations of lookup3 and MurmurHash2 for Product and String, which might be useful to replace the existing ScalaRunTime._hashCode.


(In r23229) Introduced -Ymurmur with murmur hashcodes.  Implementation primarily
contributed by "archontophoenix", following in the grand tradition
of code by people whose actual names I don't know.  References SI-2537,
but it doesn't close until some sensible hashcode is used by default.
Review by community.
I've had a chance to look at this now, and I have good news and bad news.

The good news is that not only is it possible to speed up Murmur2 as applied to
case classes, but it appears you can make Murmur2 substantially
faster than the current default hash code.

The bad news is that the amount of work required is nontrivial and
not readily generalizable
to other hash algorithms, should one be found that is preferable to Murmur2.

The other news is that Murmur2 is not necessarily the ideal target;
since making the entry above, I've benchmarked Murmur2 and Bob Jenkins's
lookup3 on a couple of other machines, and at least on an Atom processor,
lookup3 is faster than Murmur2 (by about the same ratio as Murmur2 is faster
than lookup3 on the other machines where I tested it).
It may be that the Atom is not representative of the sort of machine where
people run Scala code, but it's good to keep in mind that performance is a
slippery thing, and always context-dependent.

== How to Make It Fast: an Idea ==

The key to speeding up a structural (that is, not identity-based) hash
algorithm is to recognize it as a kind of fold. That is, you'd like to add to
Product something like:

{code}
    def foldProductElementsLeft [B] (zero: B) (op: (B, Any) => B): B = {
      var x = zero
      for (a <- productIterator)
        x = op(x,a)
      x
    }
{code}

and apply it to your favorite hash function (such as
`(h: Int, x: Any) => h * 41 + (if (x == null) 0 else x.##)`,
if you want the original case class hash function).

However, if you make the _compiler_ generate
`foldProductElementsLeft` for each case class,
it can unroll the loop. For example, for:

{code}
  case class C (
      z: Boolean, b: Byte, c: Char, s: Short, i: Int, l: Long, f: Float,
      d: Double, str: String)
{code}

you could generate:

{code}
    def foldProductElementsLeft [B] (zero: B) (op: (B, Any) => B): B = {
      var x = zero
      x = op(x,z)
      x = op(x,b)
      x = op(x,c)
      x = op(x,s)
      x = op(x,i)
      x = op(x,l)
      x = op(x,f)
      x = op(x,d)
      x = op(x,str)
      x
    }
{code}

For the unrolled version of `foldProductElementsLeft`,
a sufficiently smart VM should
be able to elide the implied boxing and unboxing, which might be a good
deal harder in the loop-based hand-written version, unless the VM itself is
clever enough to unroll the loop.

If you think your VM is dumb as a board and you want to lead it by the nose,
you can explicitly eliminate boxing by reimagining a `Product` as a sequence of
field hash codes:

{code}
    def foldProductHashCodesLeft (zero: Int) (op: (Int, Int) => Int): Int = {
      var x = zero
      x = op(x,z.##)
      x = op(x,b.##)
      x = op(x,c.##)
      x = op(x,s.##)
      x = op(x,i.##)
      x = op(x,l.##)
      x = op(x,f.##)
      x = op(x,d.##)
      x = op(x,if (str == null) 0 else str.##)
      x
    }
{code}

But this is screwy in several ways:

 * You really want to hash the raw bits of each non-reference field, not its
  `##`, even though the latter is usually the same.

 * There isn't any point in hashing the leading zeros of primitive types smaller
  than `Int`; if possible, you should consolidate the smaller fields into
  `Int`s.

 * `Int` isn't necessarily the proper quantum for all hash algorithms. lookup3
  wants its input in 96-bit chunks, not 32-bit.

Addressing the first two problems yields something like:

{code}
    def foldProductRawBitsLeft (zero: Int) (op: (Int, Int) => Int): Int = {
      var x = zero
      x = op(x,(if (z) 1 << 16 else 0) | c)
      x = op(x,(b << 16) | (s & 0xffff))
      x = op(x,i)
      x = op(x,l.toInt)
      x = op(x,(l >>> 32).toInt)
      x = op(x,java.lang.Float.floatToRawIntBits(f))
      val dd = java.lang.Double.doubleToRawLongBits(d)
      x = op(x,dd.toInt)
      x = op(x,(dd >>> 32).toInt)
      x = op(x,if (str == null) 0 else str.##)
      x
    }
{code}

I have no clue how to address the last problem in any kind of general way
while avoiding object allocation. The best I can come up with is to generate
distinct fold methods for each hash quantum desired, but because
there's no guarantee
that some yet-to-be-invented superior hash function will want to consume data
in chunks of 32 or 96 bits like the functions we have now, this is a potentially
endless task.

== Idea Meets Cruel Reality: Benchmark Results ==

The bad news is that the JVM (1.6.0_21 for 32-bit Windows,
both client and server) is dumb as a board.
On my machine, for instances of the class `C` above compiled under
scala-2.9.0.r23230-b20101012020216,
times for hash algorithms that allocate short-lived objects appear to be
roughly proportional to the number of objects allocated, suggesting that
the JVM fails to eliminate any of the allocations.
In particular, a hash function based on a general-purpose fold is considerably
slower than the `Int`-based folds (and slower than the current default).

The good news is that if you do break down and generate an `Int`-based fold,
the fastest implementation of Murmur2
(the one based on `foldProductRawBitsLeft` above) benchmarks 3 to 4
times as fast as the current default hash algorithm (combining the
current hash function with `foldProductRawBitsLeft` is faster still, since
the former does fewer operations, but the result is still poorly distributed).

You can use the attached `HashSpeed.scala` to see whether timings on your
machine resemble mine.

I don't know how the timings in the benchmark would translate to the compiler,
but I gather from the fact that you were able to observe any slowdown by
changing the hash algorithm that the compiler spends a good deal of its time
hashing.

Good stuff in there.  One thing:

> You really want to hash the raw bits of each non-reference field, not its ##, even though the latter is usually the same. 

I don't think you want to do that.  The value of ## is how you know they're equal.  If you ignore it then you will hash equal values to unequal hashcodes.

{code}
scala> case class A[T](x: T)
defined class A

scala> A(1.0f)
res0: A[Float] = A(1.0)

scala> A(1)
res1: A[Int] = A(1)

scala> res0 == res1
res2: Boolean = true  
{code}
I imagine I don't need to demonstrate that the raw bits of 1.0f and 1 are not identical, but:

{code}
scala> java.lang.Float.floatToRawIntBits(1.0f)
res3: Int = 1065353216

scala> 1.0f.##
res4: Int = 1

scala> 1.##
res5: Int = 1

scala> 1.0f.hashCode
res6: Int = 1065353216
{code}

Yes, of course, you're right. Hashing is so much fun I forget that numbers
aren't just bit patterns, but have actual semantics.

So a case class probably needs a hash code based on something like
`foldProductHashCodesLeft` rather than `foldProductRawBitsLeft` (unless you
would rather postpone thinking about this until VMs are good enough to make
an implementation based on `foldProductElementsLeft` perform reasonably, if
that ever happens).

On a related note, though, there may be room for improvement in the hash code
for `Long` in `ScalaRunTime`. That is computed (as of
scala-2.9.0.r23230-b20101012020216) as:

{code}
  @inline def hash(lv: Long): Int = {
    val iv = lv.toInt
    if (iv == lv) iv else lv.hashCode
  }
{code}

The following formulation gives the same answer as the current function for
the `Int` range (as required), but runs faster on my machine:

{code}
  @inline def hash(lv: Long): Int = {
    val low = lv.toInt
    val lowSign = low >>> 31
    val high = (lv >>> 32).toInt
    low ^ (high + lowSign)
  }
{code}

In my experience, a conditional branch takes several times as long as a shift or
add, which I believe is typical of modern processors.

(Whether such a simple hash function for `Long` (or `Int`, for that matter) is
advisable at all depends on whether you're using the value as a counter or as a
collection of (possibly poorly distributed) bit fields, but that's a whole other
topic.)

There is also an issue for the hash code for `Double`, but as it appears to be an
actual bug, I have opened Ticket SI-3957 for it.

Here is a patch that fixes this bug, as well as similar bugs in hash
values for collections.  I did _not_ put this under -Ymurmur, since
it involves a number of changes to the library to fix other equally
bad hashing problems there.

This patch is _not_ ideal for speed, since it would take me prohibitively
long to figure out how to correctly generate a synthetic method for case
classes.  For reference, case classes should generate the equivalent of
the following code:

{code}
  import scala.util.MurmurHash3._
  var h: Int = startHash( MyCaseClass.hashSeed )
  h = nextHash(h, firstParam.##, storedMagicA(0), storedMagicB(0))
  ...
  h = nextHash(h, lastParam.##, storedMagicA(n-1), storedMagicB(n-1))
  finalizeHash(h)
{code}

Note that the companion object should store a {{hashSeed}} for the case class.  It should probably hash the string of the case class name

(It might be even faster to generate the magic streams of integers in
the compiler and put the constants into the code.)

I am happy to review someone else's work in this regard.  But given that
I don't have enough spare time to learn that much about how to generate
a synthetic method with nontrivial parse trees, I've just updated the
generic method for Product.

This is still plenty fast, I think.  The collections are all faster,
and we've only lost about 30% with small tuples.  And everything is
much, much better distributed.


Benchmarks:
  1. Compiler build speed test (shorter is better)
    a. Pre-patch (24065) ant clean ; ant = 4m 33s +- 1
    a. Post-patch                        = 4m 29s +- 1
    a. Pre-patch initial build on slow scala-free machine = 57m 01s
    a. Post-patch                                         = 52m 53s

  2. Accuracy tests (lower is better)
    a. 2M 4-char strings (not replaced, could be called by user)
      i. Default collision rate = 70.44%
      i. New collision rate     =  0.03%
    a. 1M lists of lists of binary values
      i. Pre-patch collision rate = 92.43%
      i. Post-patch               =  0.01%
    a. 1M sets which are subsets of the numbers 1 to 20
      i. Pre-patch collision rate = 99.98%
      i. Post-patch               =  6.88%
    a. 2M nested tuples
      i. Pre-patch collision rate = 60.27%
      i. Post-patch               =  0.03%

  3. Speed tests (higher is better)
     a. Lists of lists of binary values
       i. Pre-patch hash rate = 3.6 M/s  +- 0.1
       i. Post-patch          = 4.8 M/s  +- 0.2
     a. Subsets of {1,...,20}
       i. Pre-patch hash rate = 2.0 M/s  +- 0.1
       i. Post-patch          = 2.6 M/s  +- 0.3
     a. Random small nested tuples
       i. Pre-patch hash rate = 3.5 M/s  +- 0.1
       i. Post-patch          = 2.8 M/s  +- 0.2

Two files are attached.

mm3_patch_24071.diff can be used to patch the code.  It's made against 24071 (which doesn't actually build on my system; I built against 24065, but that shouldn't matter since the relevant code hasn't been changed since).

HashSpeedTest.scala can be used to look at the hashing performance of different implementations.  It is memory-hungry, as it builds large sets of objects to hash before hashing them (object creation is generally slower than hashing, so I wanted to separate the two); I ran it with -Xmx6G.

A couple notes for your possible edification.  The private vals in MurmurHash are constant folded if you mark them final.  This seems like something you of all people would know, so maybe there's a reason you didn't? It knocks about 7% off the size of the MurmurHash$$ bytecode, to say nothing of whatever it does for performance.  (Hard to imagine it hurts.)

Also, you translated a line like this:
{code}
-      code = code * 41 + (if (elem == null) 0 else elem.##)
+      h = extendHash(h, if (elem.asInstanceOf[AnyRef] eq null) 0 else elem.##, c, k)
{code}
The "if (elem == null)" generates identical bytecode.  I can only imagine rewriting it like that if I thought otherwise.

I will be checking something in soon.
(In r24083) A new murmur3 hashcode implementation contributed by rex kerr.
I'm really happy with it.  There is benchmarking code included
but you can use the pudding for proof: the compiler compiling
itself is clearly faster.  I deleted the existing murmur2
implementation which has never left trunk in a release.

There remain some possible points of improvement with inlining
and/or synthetic method generation, but this is a major improvement
over the status quo.  Closes SI-2537, review by prokopec.
Replying to [comment:27 extempore]:
> Note to archontophoenix/adrian: I hope you don't feel unappreciated here

I don't feel unappreciated at all; I'm grateful that someone else took an interest, too, and I look forward to using the better-distributed hash codes.

I'd be in a lot of trouble if the only bug fixes I took advantage of were the ones I came up with myself.
