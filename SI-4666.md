ScalaSignature contains different bytes compared to what ScalaSigParser.parse(clazz) returns at least for some classes in standard library:

{code}
Compare ScalaAnnotation to ScalaSig attribute for scala.Predef
04824: 0xff != 0x7f
07098: 0xff != 0x7f
08288: 0xff != 0x7f

Compare ScalaAnnotation to ScalaSig attribute for scala.package
02458: 0xff != 0x3f
02920: 0xff != 0x7f
04159: 0xff != 0x7f
05678: 0xff != 0x3f
07504: 0xff != 0x7f
08890: 0xff != 0x7f
09598: 0xff != 0x7f
10221: 0xff != 0x3f
{code}

This is how I get both byte arrays:
{code}
val clazz = Class.forName(className)
val bytes = clazz.getAnnotation(classOf[ScalaSignature]).bytes.getBytes("UTF-8")
val len = ByteCodecs.decode(bytes)

val scalaSig = ScalaSigParser.parse(clazz).get
val bytesAttr = scalaSig.getEntry(0).byteCode.bytes
{code}

Obviously, bytes from annotation are unparseable.
I think I've found the problem: ByteCodecs expects bytes in modified UTF-8 encoding, used in Java class files. Annotation string is encoded in pure UTF-8 (not modified). The only interesting difference between this formats is encoding of 0x00, but since it doesn't appear so often, this difference is easily overlooked.

I think it is a good idea to add decodeUTF8(bytes: Array[Byte]) to ByteCodecs and document the difference with decode(..). Implementation might look like this:

{code}
def min1(src: Array[Byte]) {
  var i = 0
  val srclen = src.length
  while (i < srclen) {
    val in: Int = src(i) & 0xff
    if (in == 0x00) {
      src(i) = 0x7f
    } else {
      src(i) = (in - 1).toByte
    }
    i += 1
  }
}

def decodeUTF8(xs: Array[Byte]): Int = {
  min1(xs)
  decode7to8(xs, xs.length)
}
{code}
