
The hypothesis was that `typer` was going to be the main source of races. Thus I picked `dce` , an optimization phase  that does most of its work without constantly invoking `Tree.tpe` or `Symbol.info` (relatively speaking). The resulting speedup is almost linear (with the caveat of outliers, discussed in the next comment). 

The parallel version is structured around a priority work and a thread pool (plain `java.util.concurrent`). The resulting scheduling seems just right.

That structure can be gleaned from [{{DeadCodeElimination.scala}} ](https://github.com/magarciaEPFL/scala/blob/parallelDCE/src/compiler/scala/tools/nsc/backend/opt/DeadCodeElimination.scala) .  See [change summary ](https://github.com/magarciaEPFL/scala/compare/master...parallelDCE) at GitHub.

Basically the differences with the sequential version are:

- making mutable-shared-state not shared across threads (e.g., `Linearizer` and `Peephole` are now instance-level and thus not shared across tasks submitted to Executor)
- priority queue (containing units for work, longest works first, as coarse-grained jobs as possible)
- fixed-size pool for worker threads (not larger than available cores).
- synchronization is encapsulated in DCE (ie no modifications are required outside DCE). For example, `Instruction.produced` invokes `typer` (at least for some overrides). A working solution is:
```
    private def getProduced(i: Instruction): Int = {
      if(i.isInstanceOf[opcodes.CALL_METHOD]) {
        global synchronized i.produced // locked because CALL_METHOD.produced() invokes producedType
      } else i.produced
    }
```

Regarding speedup, the charts below show a single outlier makes the speedup sublinear. In detail, the smallest unit of work is an `IMethod`, and it so happens that `CommentFactory.parse0$1` takes up 7 sec to process. By that time all other threads are done with their thousands and thousands of `IMethod` and are sitting idle.

!visualvm.png!

!workload-distrib.png!
