Current BitSetLike implementation for the foreach function loops through each bits in the array. When the set is not dense, it is possible to avoid blocks of 64 bits by comparing them to 0L.

Using different density, on my computer I obtained a performance improvement of a factor above 5 for density below 0.1 and a minimal improvement of 20% when all bits are set.
Runtime is computed using System.nanoTime and is the mean of 20 iterations.

Note: this change might affect some program behavior. Using this implementation, if the called function insert an integer having a higher value than the current value and being stored in the same long value, then it will not be considered. E.g., inserting 42 when looping on 40. 
Since manipulating a collection while iterating over it is a bad practice and is known to produce unwanted effects, I don't if it is a problem.

I did not compare runtime using other architectures.
The best performance improvement I can get is about 40% by replacing

```
  override def foreach[B](f: Int => B) {
    for (i <- 0 until nwords) {
      val w = word(i)
      for (j <- i * WordLength until (i + 1) * WordLength) {
        if ((w & (1L << j)) != 0L) f(j)
      }
    }
  }
```

with

```
  override def foreach[B](f: Int => B) {
    for (i <- 0 until nwords) {
      var w = word(i)
      var j = i * WordLength
      while (w != 0) {
        if ((w & 1) != 0) f(j)
        w = w >>> 1
        j += 1
      }
    }
  }
```

and making sure that `WordLength` is a `final val` (which it is not right now).

Making the outer `for` loop a `while` also improves things a little presently but I expect optimizer improvements to fix this soon.
This solution improves runtime since long values having no bit sets are avoided. However, with this solution, all bits of a long value having a single bit set are still enumerated. Using {code:java}java.lang.Long.numberOfTrailingZeros{code} allows to consider only the bits which are set. I believe this method call is implemented as a single assembly instruction in most CPU architectures.

I compared both solutions by considering different density of bits in a long value. Surprisingly, on my computer, even when all bits are set, the first solution is faster (using a while loop and final val for both solutions).

I agree that the while loop improves runtime compared to the for loop. I think the reason is because a Range instance is created. It would be great if this could be optimized.

Did you get better performances using the proposed solution compared to the following ? If so I would be very interested in the benchmark.

```java
override def foreach[B](f: Int => B) {
  var i = 0
  while (i != nwords) {
    if (elems(i) != 0L) {
      var l = elems(i)
      do {
        f((i << LogWL) + java.lang.Long.numberOfTrailingZeros(l))
        l &= l - 1
      } while (l != 0L)
    }
    i += 1
  }
}
```
In my hands that's a little faster for 50% full bitsets, but only a few percent.  It's substantially faster for nearly empty bitsets, but slower (again by a few percent) for nearly full bitsets.  Mostly I'm worried about architectures where numberOfTrailingZeros is not fast, though.  If you can supply documentation showing for which architectures it's fast (would be nice to know for ARM, Power5, and Sparc in addition to Intel) *and* that it is/can be implemented in JavaScript approximately as efficiently as a bit shift, then I'll switch.  Otherwise you end up with approximately 6x more work to implement it via a series of bit masks.  I don't think it's advisable to adopt a solution that may have big performance problems in areas that would be nice to move into (Android, JavaScript backend, etc.).
