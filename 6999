> Shouldn't the compiler scale to handle any number of implicits linearly?

Short of some pretty impressive theoretical breakthroughs which overturn most of modern mathematics, I think it's unlikely.  Regardless, we should stick to talking about what the compiler does do, not what it should do.  It isn't just the number of implicits you define in that file: you have thousands and thousands of gratuitous implicit conversions taking place, and then expect the types to be inferred.  It's going to take way longer than necessary even in the best case.
