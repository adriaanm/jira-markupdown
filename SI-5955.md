The snapshots below depict system load while running `ant test.suite`. Although coarse, they suggest that over half of the test.suite run is I/O bound, for the number of tasks running in parallel, unlike the first quarter or so which is CPU bound. 

The data:

- snapshot 1: Except for a small bump all CPUs are fully loaded during `files/pos` and `files/neg` tests.
- snapshot 2: By the time `files/run` tests are being run, the CPUs have little to do during the second half of that interval.
- snapshot 3: CPU utilization recovers to around 100% (for a short while) after the batch of `files/jvm` tests start, then stays at values below 20%. With the `buildmanager` tests it recovers again, followed by another decline (the decline lasts longer than the recovery). Same pattern with the `scalacheck` tests
- snapshot 4: Finally, `files/scalap`, `files/specialized`, and `files/presentation` tests run at 50% utilization.

Perhaps we can get shorter wall-clock time by running two partests in parallel (don't laugh), mixing I/O bound and CPU bound loads. Additionally, spawning 8 parallel tasks underutilizes some cores on some machines.

In any case, any solution which doesn't require meddling with partest internals seems better than the alternatives.

There you go:

DirectRunner.scala:

```scala
    val workers = kindFiles.grouped(groupSize).toList map { toTest =>
      val worker = new Worker(fileManager, TestRunParams(scalaCheckParentClassLoader))
      worker.start()
      worker ! RunTests(kind, toTest)
      worker
    }
```


Worker.scala:

```scala
  def act() {
    react {
      case RunTests(testKind, files) =>
        val master = sender
        kind = testKind
        runTests(files) { results =>
          master ! Results(results.toMap)
          resetAll()
        }
    }
  }
```
Yes, I/O can't be the main explanation for the low CPU utilization, workload division is more likely. 
Although classloading (unpickling, reflection) and classwriting (GenJVM, less so for GenASM) take time.

Regarding even division of the workload:
- jvisualvm reveals (picture below) most worker threads in the ANT process waiting for something (yellow). 
I'm talking about "{{ForkjoinPool-1-worker-1}}" to "{{ForkjoinPool-1-worker-12}}". 
Are they partest's actors?
- Not sure about the distribution of running times for tests (that is, once they've been compiled, where most of the I/O takes place). 
After "visual inspection" the longest I noticed was ctrie taking a minute and a half. 
Doesn't seem that a few tests monopolize the run.

Another observation: run "{{partest --run}}" and jvisualvm. Do you see those Scala logos on the top left? 
Those are tests running in parallel. Select "Show live threads only" to keep the timeline uncluttered. 
Except at the beginning, there are about 5 logos at any given time. That's too low, even on an 8-core machine.

!jvisualvm.png!

Yes, that explains why `pos` and `neg` stay at 100% CPU utilization. 

- Starting dedicated JVMs avoids races in scalac, but that doesn't mean we should wait with idle CPUs for that to finish (more workers can't saturate the I/O bandwith, rather improve throughput instead). 
- Looks like the granularity of tests (except `pos` and `neg`) is such that they could be taken one at a time from a queue (a single queue actually). Together with the above we would get optimal throughput then.

I'm trying to increase the number of worker threads, like this:

```
./partest -Dactors.corePoolSize=32 -Dpartest.actors=32 -Dactors.maxPoolSize=64 -Dactors.enableForkJoin=true --run --debug
```

Output:

```
Info: actors.corePoolSize not defined

Testing interpreter and backend
Info: initializing scala.actors.Scheduler$@10d09ad3...
Info: scala.actors.scheduler.ThreadPoolConfig$@250d593e: java.version = 1.6.0_30
Info: scala.actors.scheduler.ThreadPoolConfig$@250d593e: java.vm.vendor = Sun Microsystems Inc.
Info: scala.actors.scheduler.ForkJoinScheduler@4026e9f9: parallelism 12
Info: scala.actors.Scheduler$@10d09ad3: 
      starting new scala.actors.scheduler.ForkJoinScheduler@4026e9f9 [class scala.actors.scheduler.ForkJoinScheduler]
```

Highlights:
- how can `actors.corePoolSize` be set?
# how can the {{ForkJoinScheduler}}'s parallelism be raised? (in the snippet above, {{12}}).

It's strange. When running `NestRunner` all by itself CPU usage stays all the time close to 100% , unlike when running `ant test.suite`.

A few data points (8-core machine, no warm-up, no averaging over multiple runs, relative times are what counts):
- Baseline: running `partest --run` results in
```
All of 986 tests were successful (elapsed time: 00:13:13)
```
with 4 GB peak memory use.
- running `scala.tools.partest.nest.NestRunner --run` with 
```
-Dactors.corePoolSize=32 -Dpartest.actors=32 -Dactors.maxPoolSize=64 -Dactors.enableForkJoin=true
```
results in
```
All of 986 tests were successful (elapsed time: 00:15:56)
```
with 5 GB peak memory use.
- running `NestRunner --run` with only `-Dactors.enableForkJoin=false` results in
```
All of 986 tests were successful (elapsed time: 00:13:09)
```
with 6 GB peak memory use. Regarding pool size, `--debug` informs us that:
```
Info: actors.corePoolSize not defined

Testing interpreter and backend
Info: initializing scala.actors.Scheduler$@351563ff...
Info: Thread[Thread-1,5,main]: corePoolSize = 12, maxPoolSize = 256
Info: scala.actors.Scheduler$@351563ff: starting new Thread[Thread-1,5,main] 
      [class scala.actors.scheduler.ResizableThreadPoolScheduler]
```

Background info, quoting from `scala.actors.Actor`

```
 *  If actors use blocking operations (for example, methods for blocking I/O),
 *  there are several options:
 *
 *  - The run-time system can be configured to use a larger thread pool size
 *    (for example, by setting the `actors.corePoolSize` JVM property).
 *  - The `scheduler` method of the `Actor` trait can be overridden to return a
 *    `ResizableThreadPoolScheduler`, which resizes its thread pool to
 *    avoid starvation caused by actors that invoke arbitrary blocking methods.
 *  - The `actors.enableForkJoin` JVM property can be set to `false`, in which
 *    case a `ResizableThreadPoolScheduler` is used by default to execute actors.
```

Summing up:

|| `NestRunner --run` ||elapsed time||
| ForkJoinScheduler, parallelism == 12 | 13 minutes |
| ForkJoinScheduler, parallelism == 32 | 16 minutes |
| ResizableThreadPoolScheduler         | 13 minutes |

The following bash script keeps CPU utilization close to 100% (except at the very end) shaving 5 minutes off the test suite on an 8-core machine:

```
./partest --pos --neg --jvm --res --buildmanager --scalacheck --script --shootout --presentation & 
./partest --run
wait   # for subshells to finish.
```

With 16 cores a viable partitioning would be:
- `--pos --neg`
- `--jvm --res --buildmanager --scalacheck --script --shootout --presentation`
- `--run`
I hadn't seen this ticket.  You guys might want to know that at some point I ripped the actors out of partest and deleted the pointless layers of indirection.  Like miguel says,
```
    Test suite time on my 8-core machine goes from 28:21 to 22:34.
```
I don't remember the condition of this branch but I pushed it in case anyone is interested.

  https://github.com/paulp/scala/tree/topic/partest-no-actors
Maybe THIS will fill you with excitement and new motivation:
```
BUILD SUCCESSFUL
Total time: 23 minutes 36 seconds
```
PR discussion at https://github.com/scala/scala/pull/870 

Merged in [e5c3d78b5d25a4d5356bf58cd5d3f1d41b55ee12 ](https://github.com/scala/scala/commit/e5c3d78b5d25a4d5356bf58cd5d3f1d41b55ee12)

