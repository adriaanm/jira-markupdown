Hi,

I created a small program to use Spark/Hadoop with JDBC and it ran fine on Windows 7 with JDK 8 from Oracle and JDK 7.1 from IBM. 
I ran the scala program with java -cp myscalapgm.jar;spark-assembly-1.4.1-hadoop2.6.0.jar;jccdriver.jar myscala.Test
Maybe thats not a good approach but it works on Windows. Now I have moved all the jars to z/OS (Bigendian platform) and get the exception below. A hello world scala works with no problem, but the Spark seams more complex in terms of scala usage.

During execution on z/OS I get the following stacktrace:
{noformat}
 java.lang.ExceptionInInitializerError
  at java.lang.J9VMInternals.ensureError(J9VMInternals.java:134)
  at java.lang.J9VMInternals.recordInitializationFailure(J9VMInternals.java:123)
  at org.apache.spark.sql.jdbc.JDBCRDD$.org$apache$spark$sql$jdbc$JDBCRDD$$getCatalystType(JDBCRDD.scala:62)
  at org.apache.spark.sql.jdbc.JDBCRDD$$anonfun$1.apply(JDBCRDD.scala:137)
  at org.apache.spark.sql.jdbc.JDBCRDD$$anonfun$1.apply(JDBCRDD.scala:137)
  at scala.Option.getOrElse(Option.scala:120)
  at org.apache.spark.sql.jdbc.JDBCRDD$.resolveTable(JDBCRDD.scala:136)
  at org.apache.spark.sql.jdbc.JDBCRelation.<init>(JDBCRelation.scala:128)
  at org.apache.spark.sql.jdbc.DefaultSource.createRelation(JDBCRelation.scala:113)
  at org.apache.spark.sql.sources.ResolvedDataSource$.apply(ddl.scala:269)
  at org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:114)
  at com.ibm.imstest.IMSAccessSample$delayedInit$body.apply(IMSAccessSample.scala:26)
  at scala.Function0$class.apply$mcV$sp(Function0.scala:40)
  at scala.runtime.AbstractFunction0.apply$mcV$sp(AbstractFunction0.scala:12)
  at scala.App$$anonfun$main$1.apply(App.scala:71)
  at scala.App$$anonfun$main$1.apply(App.scala:71)
  at scala.collection.immutable.List.foreach(List.scala:318)
  at scala.collection.generic.TraversableForwarder$class.foreach(TraversableForwarder.scala:32)
  at scala.App$class.main(App.scala:71)
  at com.ibm.imstest.IMSAccessSample$.main(IMSAccessSample.scala:8)
  at com.ibm.imstest.IMSAccessSample.main(IMSAccessSample.scala)
 Caused by: scala.reflect.internal.MissingRequirementError: error while loading package, Scala signature package has
 wrong version
  expected: 5.0
  found: 45.0 in scala.package
  at scala.reflect.internal.MissingRequirementError$.signal(MissingRequirementError.scala:16)
  at scala.reflect.runtime.JavaMirrors$JavaMirror.handleError$1(JavaMirrors.scala:535)
  at scala.reflect.runtime.JavaMirrors$JavaMirror.unpickleClass(JavaMirrors.scala:584)
  at scala.reflect.runtime.SymbolLoaders$TopClassCompleter.complete(SymbolLoaders.scala:32)
  at scala.reflect.internal.Symbols$Symbol.info(Symbols.scala:1231)
  at scala.reflect.internal.SymbolTable.openPackageModule(SymbolTable.scala:244)
  at scala.reflect.internal.SymbolTable.openPackageModule(SymbolTable.scala:300)
  at scala.reflect.runtime.SymbolLoaders$LazyPackageType.complete(SymbolLoaders.scala:89)
  at scala.reflect.internal.Symbols$Symbol.info(Symbols.scala:1231)
  at scala.reflect.internal.Definitions$DefinitionsClass.AnyValClass$lzycompute(Definitions.scala:275)
  at scala.reflect.internal.Definitions$DefinitionsClass.AnyValClass(Definitions.scala:275)
  at scala.reflect.runtime.JavaMirrors$class.init(JavaMirrors.scala:50)
  at scala.reflect.runtime.JavaUniverse.init(JavaUniverse.scala:12)
  at scala.reflect.runtime.JavaUniverse.<init>(JavaUniverse.scala:26)
  at scala.reflect.runtime.package$.universe$lzycompute(package.scala:16)
  at scala.reflect.runtime.package$.universe(package.scala:16)
  at org.apache.spark.sql.types.AtomicType.<init>(DataType.scala:95)
  at org.apache.spark.sql.types.StringType.<init>(StringType.scala:33)
  at org.apache.spark.sql.types.StringType$.<init>(StringType.scala:49)
  at org.apache.spark.sql.types.StringType$.<clinit>(StringType.scala)
  ... 19 more
{noformat}


This is the source code, the crash happens at sqlContext.read...
{code}
import org.apache.spark.SparkConf
import org.apache.spark.SparkContext

import org.apache.spark.sql.SQLContext

object IMSAccessSample extends App {
val conf = new SparkConf()
.setMaster("local[1]")
.setAppName("GetStokStat")
.set("spark.executor.memory", "1g")

val sc = new SparkContext(conf)

val sqlContext = new SQLContext(sc)

val optionsStokStat = scala.collection.mutable.Map[String, String]();
optionsStokStat.put("driver", "com.ibm.ims.jdbc.IMSDriver");
optionsStokStat.put("url", "jdbc:ims://172.16.36.226:5559/class://com.ibm.ims.db.databaseviews.DFSSAM09DatabaseView:user=gaebler;password=password;");
optionsStokStat.put("dbtable", "STOKSTAT");

val stokStat = sqlContext.read.format("jdbc").options(optionsStokStat).load();    
{code}

I have opened a problem for the IBM JDK on z/OS and they state that the problem happens in the unpickleClass method:
{noformat}
  at scala.reflect.runtime.JavaMirrors$JavaMirror.unpickleClass(JavaMirrors.scala:584)
{noformat}

For whatever reason value 45.0 is returned, although the {{javap -verbose}} output shows the following:
{noformat}
public final class scala.package                                                
  SourceFile: "package.scala"                                                   
  InnerClasses:                                                                 
       public static #10 of #2; //class scala/package$$anon$1 of class scala/pac
kage                                                                            
       public static #15= #12 of #14; //$hash$colon$colon$=class scala/collectio
n/immutable/Stream$$hash$colon$colon$ of class scala/collection/immutable/Stream
  RuntimeVisibleAnnotations:                                                    
    0: #6(#7=s#8)                                                               
    ScalaSig: length = 0x3                                                      
     05 00 00                                                                   
  minor version: 0                                                              
  major version: 50                                                             
  flags: ACC_PUBLIC, ACC_FINAL, ACC_SUPER                                       
Constant pool:                                                                  
    #1 = Utf8               scala/package                                       
    #2 = Class              #1            //  scala/package                     
    #3 = Utf8               java/lang/Object                                    
    #4 = Class              #3            //  java/lang/Object                  
    #5 = Utf8               package.scala                                       
    #6 = Utf8               Lscala/reflect/ScalaSignature;                      
    #7 = Utf8               bytes                                               
{noformat}
...
So I don't really think its an endianess problem, but it seams only to occur on z/OS and I have no idea how to fix it, if its a JVM bug on z/OS I need something that I can give the IBM JDK support to work with, e.g. a Bytearrayinputstream with 2 more bytes in it or something like that. The must be a reason, that the unpickleClass does not return the 5 as its shown by the javap command.

Thanks for helping, it would be great to get scala to work on z/OS.
Denis.
