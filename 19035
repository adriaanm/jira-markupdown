There is a bug in the scaladoc generation that is preventing us from making an important API change in Spark.

We are using constructor overloading in Spark's flagship class "SparkContext" and when generating our docs the compilation phase fails even though a normal compile is fine. Weirdly, if I change our class constructor to have a default argument, then the compilation succeeds.

Our class SparkContext has the following constructors:
```scala
// Class constructor
class SparkContext(config: SparkConf)

// Constructor 1
def this(config: SparkConf, preferredNodeLocationData: Map[String, Set[SplitInfo]]) = {

// Constructor 2
def this(master: String, appName: String, conf: SparkConf)

// Constructor 3
 def this(
      master: String,
      appName: String,
      sparkHome: String = null,
      jars: Seq[String] = Nil,
      environment: Map[String, String] = Map(),
      preferredNodeLocationData: Map[String, Set[SplitInfo]] = Map())
```

When compiling docs, all uses of Constructor 3 fail that make use of the default arguments. An example of a failure message is:
```scala
[error] /home/patrick/Documents/spark/core/src/main/scala/org/apache/spark/api/java/JavaSparkContext.scala:70: not enough arguments for constructor SparkContext: (master: String, appName: String, sparkHome: String, jars: Seq[String], environment: scala.collection.Map[String,String], preferredNodeLocationData: scala.collection.Map[String,scala.collection.Set[org.apache.spark.scheduler.SplitInfo]])org.apache.spark.SparkContext
[error]     this(new SparkContext(master, appName, sparkHome, Seq(jarFile)))
```

Strangely, if I change the default constructor to have a default argument.  The doc compile starts working:
```scala
// Existing class constructor
class SparkContext(config: SparkConf)
// Doing this makes the compile succeed
class SparkContext(config: SparkConf, foo: Int = 1)
```

This is not an acceptable workaround for us though, as it obfuscates our API.
