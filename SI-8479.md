There is a bug in the scaladoc generation that is preventing us from making an important API change in Spark.

We are using constructor overloading in Spark's flagship class "SparkContext" and when generating our docs the compilation phase fails even though a normal compile is fine. Weirdly, if I change our class constructor to have a default argument, then the compilation succeeds.

Our class SparkContext has the following constructors:
{code}
// Class constructor
class SparkContext(config: SparkConf)

// Constructor 1
def this(config: SparkConf, preferredNodeLocationData: Map[String, Set[SplitInfo]]) = {

// Constructor 2
def this(master: String, appName: String, conf: SparkConf)

// Constructor 3
 def this(
      master: String,
      appName: String,
      sparkHome: String = null,
      jars: Seq[String] = Nil,
      environment: Map[String, String] = Map(),
      preferredNodeLocationData: Map[String, Set[SplitInfo]] = Map())
{code}

When compiling docs, all uses of Constructor 3 fail that make use of the default arguments. An example of a failure message is:

{code}
[error] /home/patrick/Documents/spark/core/src/main/scala/org/apache/spark/api/java/JavaSparkContext.scala:70: not enough arguments for constructor SparkContext: (master: String, appName: String, sparkHome: String, jars: Seq[String], environment: scala.collection.Map[String,String], preferredNodeLocationData: scala.collection.Map[String,scala.collection.Set[org.apache.spark.scheduler.SplitInfo]])org.apache.spark.SparkContext
[error]     this(new SparkContext(master, appName, sparkHome, Seq(jarFile)))
{code}

Strangely, if I change the default constructor to have a default argument.  The doc compile starts working:

{code}
// Existing class constructor
class SparkContext(config: SparkConf)
// Doing this makes the compile succeed
class SparkContext(config: SparkConf, foo: Int = 1)
{code}

This is not an acceptable workaround for us though, as it obfuscates our API.

I'm not able to reproduce this with Scala 2.10.4 or 2.10.3 with:

{noformat}
  ~/code/incubator-spark git show
commit 4d880304867b55a4f2138617b30600b7fa013b14
Author: Bryn Keller <bryn.keller@intel.com>
Date:   Mon Feb 24 17:35:22 2014 -0800
{noformat}

Could you please point to a commit SHA that can reproduce the problem?
Thanks for looking at this. This exists only in a topic branch in my repo:

{code}
git clone https://github.com/pwendell/spark.git
cd spark
git checkout 7fb13b21864ac030ae3bd09d0a4262bdb5bf66f3
sbt/sbt compile # works
sbt/sbt doc # compile fails
{code}

By the way - that build will be 2.10.3. I tested 2.10.4 also and it also failed, so I reported that in the issue since it's the most recent version in which I saw a failure.
Standalone reproduction:

{code}
object Test {
  val x = new SparkContext(master = "")
}

class SparkContext(config: Any) {

  /** Scaladoc comment */
  def this(
      master: String,
      appName: String = "") = this(null)
}
{code}

{noformat}
warning: dropping dependency on node with no phase object: pickler
test.scala:2: error: not enough arguments for constructor SparkContext: (master: String, appName: String)SparkContext
  val x = new SparkContext(master = "")
          ^
{noformat}

Removing scaladoc from the constructor with defaults seems to hide the bug.
We created package private constructors that select for specific combinations of the variables that have defaults. Once this bug gets fixed, we'll just remove them and there is no change to consumers of the Spark API. The only thing I'm concerned about is that I'm surprised Scala this doesn't throw any ambiguity exceptions here. In the example below the private constructor is used even though either constructor could technically apply. Just wondering - why is this not ambiguous? Is this approach we've taken safe?

{code}
package mypackage

class Example {

  // This is our main constructor involving defaults
  def this(a: String, b: String = "foo", c: String = "bar") {
    this()
  }

  private[mypackage] def this(a: String, b: String) {
    this(a, b, "bar")
    println("called internal constructor")
  }
}

object Example {
  def main(args: Array[String]) {
    val x = new Example("a", "b")
  }
}
{code}
