This doesn't appear to be a bug per se, because when I ran the following test, it gave no output:
{code:scala}
def hashOf(key: Int) = {
  var h = key
  h ^= ((h >>> 20) ^ (h >>> 12))
  h ^ (h >>> 7) ^ (h >>> 4)
}

def check(key: Int) = {
  val hash = hashOf(key)
  val arr = new Array[Boolean](64)
  val mask = arr.length - 1

  var j = hash
  var index = hash & mask
  var perturb = index
  (0 to arr.length * 2).foreach( i => {
      arr(index) = true
      j = 5 * j + 1 + perturb
      perturb >>= 5
      index = j & mask
    })

  val falses = arr.filterNot(i => i).length
  if (falses >= arr.length / 2)
    println( key, falses, arr.mkString(",") )
}

(0 to 100000000).foreach( check(_) )
{code}

Still, I've seen no theoretical justification for why this "exponential probing" should work, nor have I seen this technique used elsewhere, _and_ it adds unnecessary computation compared to linear or quadratic probing, while having the poor locality of reference of double hashing but none of its non-clustering guarantees. So I still think it should be replaced on performance grounds.
