Double.Range produces wrong results, leaving out the last element whenever (end-start) == ((end-start) / step) * step in BigDecimals. Just browsing through the sources, I can't find any reason why BigDecimalAsIfIntegral.quot, DoubleAsIfIntegral.quot and FloatAsIfIntegral.quot are implemented with regular division operator / instead of BigDecimal.quot.

Bug example:
> Double.Range(0, 1, 0.7)
res1: NumericRange(0.0)
> Double.Range(0, 0.6, 0.25)
res2: NumericRange(0.0, 0.25)

There is a related issue from 2011 at #4985 
Whoops! Looks like a regression!

```scala
Welcome to Scala version 2.10.4 (Java HotSpot(TM) 64-Bit Server VM, Java 1.7.0_21).
Type in expressions to have them evaluated.
Type :help for more information.

scala> Range.Double(0, 1, 0.7)
res0: scala.collection.immutable.NumericRange[Double] = NumericRange(0.0, 0.7)
```

```scala
Welcome to Scala version 2.11.1 (Java HotSpot(TM) 64-Bit Server VM, Java 1.7.0_21).
Type in expressions to have them evaluated.
Type :help for more information.

scala> Range.Double(0.0, 1.0, 0.7)
res0: scala.collection.immutable.NumericRange[Double] = NumericRange(0.0)
```
The regression wasn't caught, because there are currently no "real" tests for floating point ranges where 
either start, end or step would be different from a whole integer. This is a problem especially since the current
NumericRange.count method has fully separate code for "effectively integer" ranges and so actually most of the logic
in count seems to have no tests.

Suggestion: I will add a bunch more tests for possible NumericRange pitfalls.

2.10.x seems to do num.toLong(num.quot(...)) which avoids the problem, but that was generalized away in 994de8ff.

