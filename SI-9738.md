
```scala
Set(2, 1).reduce(math.max)
```
compiles fine while 
```scala
Set(2L, 1L).reduce(math.max)
```
fails with message: 
    error: type mismatch;
     found   : (Int, Int) => Int
     required: (AnyVal, AnyVal) => AnyVal
           Set(2L, 1L).reduce(math.max)

Both should be rejected with message: ambiguous reference to overloaded definition

I'm not sure they should be rejected but I don't see why they would behave differently. According to the spec the type of the method arguments during eta-expansion is guided by the method type:

```
Second, one creates a fresh name yi for every argument type Ti of the method (i=1,…,n)
```

The implementation, however, seems to be based on the expected function type, so that overload resolution works as expected for the `Int` case:

```scala
scala.this.Predef.Set.apply[Int](2, 1).reduce[Int]({
  ((x: Int, y: Int) => scala.math.`package`.max(x, y))
})
```

This looks like the right thing to do and I think it should also work for `Long`.
> The implementation, however, seems to be based on the expected function type, so that overload resolution works as expected for the Int case:

Nope. It works for Int by coincidence. Eta-expanding an overloaded method gets you the first one it happens to find.
```scala
scala> math.round _
res3: Long => Long = <function1>

scala> math.max _
res4: (Int, Int) => Int = <function2>
```
Similarly,
```scala
scala> List[Char]('5') map math.round
<console>:12: error: type mismatch;
 found   : Long => Long
 required: Char => ?
       List[Char]('5') map math.round
                                ^
scala> List[Char]('5') map (x => math.round(x))
res0: List[Long] = List(53)
```
Overloading is picking the most specific first, which is possible because of widening conversions. OK, I guess "first one it happens to find" was tongue-in-cheek.

The expected failure would be
```scala
scala> object X { def f(i: Int, j: Int) = i + j ; def f(s: String, t: String) = s + t }
defined object X

scala> Set(1,2) reduce X.f
<console>:13: error: ambiguous reference to overloaded definition,
both method f in object X of type (s: String, t: String)String
and  method f in object X of type (i: Int, j: Int)Int
match expected type (?, ?) => ?
       Set(1,2) reduce X.f
                         ^
```


*> I'm not sure they should be rejected*

On the subject of [~szeiger]'s comment, and as discussed in a more roundabout way in https://github.com/scalapuzzlers/scalapuzzlers.github.com/issues/146#issuecomment-275896910 and subsequent comments: is it clear from the spec that this should fail?

The following code snippets (2.12.1 REPL) seem to point in different directions here.

- The compiler is able to pick a particular overloaded method to be eta-expanded if the expected function type is given:

```scala
scala> val maxLongFun: (Long, Long) => Long = math.max
maxLongFun: (Long, Long) => Long = $$Lambda$1276/401366570@52290e63
```

- The compiler cannot pick a particular overloaded method if the expected function type uses a type parameter with type bounds:

```scala
scala> def getMaxTFun[T >: Long]: (T, T) => T = math.max
<console>:12: error: type mismatch;
 found   : (Int, Int) => Long
 required: (T, T) => T
       def getMaxTFun[T >: Long]: (T, T) => T = math.max
                                                     ^
...
        def getMaxTFun[T >: Long]: (T, T) => T = {
          ((x: Int, y: Int) => scala.math.`package`.max(x.toLong, y.toLong))
...

```

So are the type bounds the problem? In the above example, overloading resolution seems to have picked the correct version of `max` - it looks as though eta expansion happened "too soon"?

As suggested by the previous example, overload resolution unrelated to eta expansion seems to be fine with type bounds:

```scala
scala> def maxT[T >: Long](a: Int, b: Int): T = math.max(a, b)
[[syntax trees at end of                     typer]] // <console>
...
        def maxT[T >: Long](a: Int, b: Int): T = scala.math.`package`.max(a.toLong, b.toLong)
...
maxT: [T >: Long](a: Int, b: Int)T
```

If (from the first snippet) the compiler can use the expected function type to guide overloading resolution before ( ?) eta expansion, and (from the third snippet) type *bounds* are sufficient to help the compiler determine the correct method during "normal" overloading resolution, why would type bounds on the expected type of the eta-expanded function be expected to fail?

/cc @som-snytt
I think 6.26.4 case 3 applies. args are typed first with type params taken as constants, then as undefined.

It doesn't say, OK, once we figure out a type arg, re-typecheck the value args with the corresponding expected type.

The cases for overload resolution and type inference are similar, in suggesting that if you can't infer the intended target with minimal information, then you're doing it wrong.
*> I think 6.26.4 case 3 applies. args are typed first with type params taken as constants, then as undefined.*

In that case, if I'm understanding this correctly, the order of actions here is:

- The compiler attempts to choose* the appropriate overloaded version of `math.max` to eta-expand. The expected type for the argument to `reduce` at this point is `(A1, A1) ⇒ A1` for `A1 >: Long`. The compiler chooses the most specific variant of `math.max`, i.e. `(Int, Int) => Int`
- As the second step, the compiler attempts to determine the type argument to `reduce` according to 6.26.4, with solution `A1 == AnyVal`

If that sequence is correct, would it be accurate to say that the choice of {{(Int, Int) => Int}} in the first step is not influenced by the type bound {{A1 >: Long}}? If so, why does the following example not also fail to compile?

```scala
def maxT[T >: Long](a: Int, b: Int): T = math.max(a, b)
```

In this second case, it appears as though the compiler either solves the type argument equation, with  result `T == Long`, *before* overload resolution, or that the type bound `T >: Long` *is* taken into account during overload resolution in a way that differs from the `reduce(math.max)` case.

I hope I'm just missing something simple here - if so, apologies for the confusion!

??*according to the final "Assume finally that e does not appear as a function..." section in 6.26.3???
In `g` below, I was surprised to see that `math.max(Int,Int)` expands to `(x: Int, y: Int) => math.max(x, y)` which then changes the overload selection to `(x: Int, y: Int) => math.max(x.toLong, y.toLong)`. Equally erroneous but somehow more natural would be `math.max(x,y).toLong`.

Applicability includes result type, but all that's required is weak conformance.

```scala
scala> def f(x: Int, y: Int): Long = math.max(x,y) //print

def f(x: scala.Int, y: scala.Int): scala.Long = scala.math.`package`.max(x, y).toLong // : <notype>

scala> def f(x: Int, y: Int): Long = math.max(x,y) //print
f: (x: Int, y: Int)Long

scala> def f[A >: Long](x: Int, y: Int): A = math.max(x,y) //print

def f[A >: scala.Long](x: scala.Int, y: scala.Int): A = scala.math.`package`.max(x.toLong, y.toLong) // : <notype>

scala> def f[A >: Long](x: Int, y: Int): A = math.max(x,y) //print
f: [A >: Long](x: Int, y: Int)A

scala> def g[A >: Long]: (A,A)=>A = math.max //print

def g[A >: scala.Long]: _root_.scala.Function2[A, A, A] = {
  ((x: Int, y: Int) => scala.math.`package`.max(x.toLong, y.toLong))
} // : <notype>

scala> def g[A >: Long]: (A,A)=>A = math.max //print
<console>:11: error: type mismatch;
 found   : (Int, Int) => Long
 required: (A, A) => A
       def g[A >: Long]: (A,A)=>A = math.max //print
                                         ^

```
There's a [SO question](http://stackoverflow.com/questions/18108965/reducing-an-array-of-float-using-scala-math-max) which I must have understood because I added the debug output to the answer. Supposedly it shows when type inference wins.
*> There's a SO question*

From the [relevant answer|http://stackoverflow.com/a/18109794]: "Using -Xlog-implicits -Yinfer-debug shows the difference between {{reduce(math.max)}}, where overload resolution happens first, and the version where the param type is solved for first"

If the order of evaluation (of overload resolution vs. type inference) can indeed be different, that would probably explain the above differences in behaviour, too.

Do we know whether there's anything in the spec that stipulates in which order these two operations should happen, or whether this is (intentionally or otherwise) left open for the compiler to decide?
A code example (2.12.1 REPL) that seems very similar to the failing `Set(2L, 1L).reduce(math.max)` example, but that (somehow?) works:

```scala
scala> def reduce[A, B >: A](s: Set[A])(fun: (B, B) => B): B = s.reduce(fun)
reduce: [A, B >: A](s: Set[A])(fun: (B, B) => B)B

scala> reduce(Set(1L))(math.max) // but then why does this work?!
res0: Long = 1
```

According to `-Xprint:typer`, it is indeed successfully picking the `(Long, Long) => Long` version of `math.max`:

```scala
private[this] val res2: Long = $line27.$read.$iw.$iw.reduce[Long, Long](
scala.Predef.Set.apply[Long](1L))({
          ((x: Long, y: Long) => scala.math.`package`.max(x, y))
        });
```
`-Ytyper-debug` shows `x.reduce(f)` types `f` with type param to `x.reduce` unsolved; but `reduce(x)(f)` already determines the type args after the first arg list, so there is an expected type for `f`.
