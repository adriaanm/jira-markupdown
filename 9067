I think we might have a problem. For certain lists the running time of the serialization becomes quadratic.
Think of a list holding objects each of which points to the tail of that list.
If you take a look at `writeObject`, you'll see that the recursive calls ensue, each of which does a while-loop to traverse all the elements.

Changing the list size above `32` results in really long running times. Not to say that it can still result in a stack overflow.
```scala
object Test {

  def deepCopy[T](obj : T, reportSize: Boolean = false): T = {
    val baos = new ByteArrayOutputStream()
    val oos = new ObjectOutputStream(baos)
    oos.writeObject(obj)
    oos.flush()
    val data = baos.toByteArray
    if (reportSize) println(data.length)
    val bais = new ByteArrayInputStream(data)
    val ois = new ObjectInputStream(bais)
    ois.readObject().asInstanceOf[T]
  }
 
  def test() {
    case class Foo(tail: List[Foo])

    def create(len: Int): List[Foo] = if (len == 0) Nil else {
      val tail = create(len - 1)
      Foo(tail) :: tail
    }

    val xs = List.fill(32)(7)
    val dxs = deepCopy(xs, true)
    assert(xs == dxs)

    val ys = create(32)
    val dys = deepCopy(ys, true)
    assert(ys == dys)
  }

  def main(args: Array[String]) {
    test()
  }

}
```
