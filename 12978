I analyzed the JVM bytecode of scala/BigInt$$ (object scala.BigInt) in the current Scala 2.6.1 and have some important remarks about the generated code when it is targeted to execute on Java 5 and later.

The object BigInt maintains a cache of small values in form of an array. The array is filled in a lazy fashion by apply(Int).

However, there are several issues in the generated bytecode with respect to the Java Memory Model (JMM) that underlies Java 5 and later. The singleton object scala.BigInt has 3 vals: minCached, maxCached and cache. The vals appear as private fields in the .class but *none* of them is final. Moreover, all accesses to the fields in the bytecode are never protected by some form of synchronization (in the sense of the JMM). This is to say that the singleton is not thread-safe.

As an example, suppose one thread executes the constructor of the singleton. The constructor sets some values in minCached and maxCached and sets cache to a newly created BigInt array. Since the fields are not final, *none* of the writes needs to be visible to other threads. Any other thread could see 0 in the int fields and null in the cache field. A null cache leads to a NullPointerException in apply().
To stress the point, it is not sufficient for a field to never be written outside a constructor to ensure that its definitive value will be seen by other threads. The only guarantee is that all other threads will at least see the default value (0, null, false) instead of some random value. (With long and double nonfinals even apparently random bit patterns can be read by other threads!)

***Vals shall be final.***
When executed on Java 5 or later, this has a vital impact on thread-safety. When run on earlier JVMs it doesn't hurt.

However, even with properly implemented vals (final fields), nothing in the current code ensures that newly cached values computed by a particular thread become visible to other threads as well. With proper vals, the only cached values that are guaranteed to be seen by all threads are the initial null values. Therefore, it may happen that each thread ends up to fill the same positions in the cache. In other words, the cache could behave like a thread local instead of being a truly global cache. While this would still be a correct behavior, it may not be optimal.

To ensure that changes done in one thread become visible to other threads, the Java Memory Model requires some form of synchronization. In the case of BigInt, this can be done by JVM volatiles or by JVM monitors. Or by using some java.concurrent classes like atomics or locks.

These have similar semantics. A volatile read, for example, has acquire semantics; a volatile write has release semantics. Release semantics means that all memory actions that, in program order, come before the volatile write are viewed as happening before a successive read on the same volatile by any other thread. Apart from the other forms of synchs described by the JMM, there is no other ordering guarantee between unsynched memory actions as seen by other threads.

By using volatiles, and with vals implemented as final fields, a better version of apply() is (sorry for possible syntax errors, I'm just a newbie in Scala)

...
  private @volatile var sync = 0; // value irrelevant
...
...
  def apply(i: Int): BigInt =
    if (minCached <= i && i <= maxCached) {
      val offset = i - minCached
      sync // acquire
      var n = cache(offset)
      if (n eq null) {
        n = new BigInt(BigInteger.valueOf(i)); cache(offset) = n;
		sync = 0; // release
      }
      n
    } else new BigInt(BigInteger.valueOf(i))
...

The combination of read/write of sync ensures that changes to the array made by any thread will be promptly visible to all other threads. Volatiles do *not* guarantee mutual exclusion. If more threads execute apply() for the same small uncached value at the same time, all can happen to compute and cache it, depending on the interplay between the volatile actions. But once a thread has successfully executed the volatile write, any other thread that subsequently executes the volatile read will see the the value cached by the former thread.

Note that during execution not all acquires are balanced by releases. This contrasts with the nesting of aquire/release patterns in the execution of monitors. Volatile reads (aquires) are quite inexpensive on current shared memory hardware and less expensive than entering a monitor. Also, volatiles do not block threads and so cannot lead to deadlocks.

Java 1.4 and earlier have a weaker form of volatiles that cannot guarantee what is discussed above. On these platform one must possibly resort to the more expensive monitors, but I'm not sure if they ensure visibility as described. However, in this particular case volatiles don't hurt either, so I would write the code as above.

(Because of the stronger guarantees of the JMM, Scala should target Java 5 or later. I'm ignorant about similar issues on .NET.)

Cheers
Raffaello

